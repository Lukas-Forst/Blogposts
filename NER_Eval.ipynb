{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbaD2aHzQy6yUN21rAy16k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lukas-Forst/Blogposts/blob/main/NER_Eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqTHUNT-JlxL"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "!pip install seqeval\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install wandb\n",
        "\n",
        "import torch\n",
        "import wandb\n",
        "wandb.init(project='ner_conll', settings=wandb.Settings(start_method=\"thread\"))\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, load_metric, Dataset, DatasetDict\n",
        "import numpy as np\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "\n",
        "def read_conll_file(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        content = f.read().strip()\n",
        "        sentences = content.split(\"\\n\\n\")\n",
        "        data = []\n",
        "        for sentence in sentences:\n",
        "            tokens = sentence.split(\"\\n\")\n",
        "            token_data = []\n",
        "            for token in tokens:\n",
        "                token_data.append(token.split())\n",
        "            data.append(token_data)\n",
        "    return data\n",
        "\n",
        "\n",
        "train_data = read_conll_file(\"/content/train.txt\")\n",
        "validation_data = read_conll_file(\"/content/valid.txt\")\n",
        "test_data = read_conll_file(\"/content/test.txt\")\n",
        "\n",
        "\n",
        "def convert_to_dataset(data, label_map):\n",
        "    formatted_data = {\"tokens\": [], \"ner_tags\": []}\n",
        "    for sentence in data:\n",
        "        tokens = [token_data[0] for token_data in sentence]\n",
        "        ner_tags = [label_map[token_data[3]] for token_data in sentence]\n",
        "        formatted_data[\"tokens\"].append(tokens)\n",
        "        formatted_data[\"ner_tags\"].append(ner_tags)\n",
        "    return Dataset.from_dict(formatted_data)\n",
        "\n",
        "\n",
        "label_list = sorted(list(set([token_data[3] for sentence in train_data for token_data in sentence])))\n",
        "label_map = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "\n",
        "train_dataset = convert_to_dataset(train_data, label_map)\n",
        "validation_dataset = convert_to_dataset(validation_data, label_map)\n",
        "test_dataset = convert_to_dataset(test_data, label_map)\n",
        "\n",
        "\n",
        "datasets = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": validation_dataset,\n",
        "    \"test\": test_dataset,\n",
        "})\n",
        "\n",
        "model_name = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
        "\n",
        "def compute_metrics(eval_prediction):\n",
        "    predictions, labels = eval_prediction\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision_score(true_labels, true_predictions),\n",
        "        \"recall\": recall_score(true_labels, true_predictions),\n",
        "        \"f1\": f1_score(true_labels, true_predictions),\n",
        "        \"classification_report\": classification_report(true_labels, true_predictions),\n",
        "    }\n",
        "\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=True\n",
        "    )\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "#!pip install accelerate -U\n",
        "\n",
        "#!pip install accelerate\n",
        "\n",
        "import accelerate\n",
        "\n",
        "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_steps=20,\n",
        "    learning_rate=5e-5,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        ")\n",
        "\n",
        "def data_collator(data):\n",
        "    input_ids = [torch.tensor(item[\"input_ids\"]) for item in data]\n",
        "    attention_mask = [torch.tensor(item[\"attention_mask\"]) for item in data]\n",
        "    labels = [torch.tensor(item[\"labels\"]) for item in data]\n",
        "\n",
        "\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# Updating W&B config\n",
        "wandb.config.update({\n",
        "    'num_train_epochs': 5,\n",
        "    'learning_rate': 5e-5,\n",
        "    'weight_decay': 0.01,\n",
        "    'per_device_train_batch_size': 8,\n",
        "    'per_device_eval_batch_size': 8,\n",
        "    'logging_dir': './logs',\n",
        "    'logging_steps': 10,\n",
        "    'eval_steps': 10,\n",
        "    'save_steps': 50,\n",
        "    'evaluation_strategy': 'steps'\n",
        "})\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "sentence = \"Apple Inc. announced that Tim Cook will be attending the event in San Francisco.\"\n",
        "\n",
        "\n",
        "tokenized_input = tokenizer(sentence, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "\n",
        "outputs = model(**tokenized_input)\n",
        "\n",
        "\n",
        "predicted_labels = outputs.logits.argmax(-1)[0]\n",
        "\n",
        "\n",
        "named_entities = [tokenizer.decode([token]) for token, label in zip(tokenized_input[\"input_ids\"][0], predicted_labels) if label != 0 and label != label_map['O']]\n",
        "\n",
        "\n",
        "print(\"Named Entities - Example 1:\", named_entities)"
      ]
    }
  ]
}